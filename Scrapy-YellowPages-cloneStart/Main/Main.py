import subprocess
import os
import pandas as pd
import customtkinter as ctk
from tkinter import messagebox, ttk


# Specify the relative path to the script you want to run
script_path = os.path.abspath('Scrapy-YellowPages-cloneStart/Scrapy-YellowPages-master/yellowp/spiders/ylp.py')

def getInitialInput():
    # Get input from the user
    profession = profession_input.get()
    location = location_input.get()
    state_code = state_input.get()

    if not profession or not location or not state_code:
        messagebox.showerror("Error", "Please fill in all fields.")
        return None, None, None
    return profession, location, state_code

def filterTable(df, locationString):
    # Ensure the CSV has basic column data
    filtered_df = df.dropna(subset=['Name', 'Phone', 'Website', 'Locality'])

    # Apply filters to optimize selection
    filtered_df = filtered_df[filtered_df['Locality'].str.contains(locationString, case=False, na=False)]
    filtered_df = filtered_df[filtered_df['TonsOfInfo'].str.contains("Business", case=False, na=False)]
    filtered_df = filtered_df[filtered_df['TonsOfInfo'].str.contains("Accredited", case=False, na=False)]
    
    return filtered_df

def runSubprocess():
    profession, location, state_code = getInitialInput()
    if profession is None:
        return  # Do nothing if input is invalid

    # Run subprocess - to scrape data
    subprocess.run(["python", script_path, profession, location, state_code]) 
    print("Scraping done...")

    try:
        # Read data generated by the subprocess
        df = pd.read_csv("Scrapy-YellowPages-cloneStart/Main/output.csv")

        # Filter data
        filtered_df = filterTable(df, location)

        # Save filtered results to CSV
        filtered_df.to_csv('Scrapy-YellowPages-cloneStart/Main/filtered_data.csv', index=False)

        # Display data in the table
        updateTable(filtered_df)

        # Clean up: Remove the original output CSV file
        os.remove("Scrapy-YellowPages-cloneStart/Main/output.csv")
    except FileNotFoundError:
        messagebox.showerror("Error", "CSV file not found. Make sure the scraper ran correctly.")

def updateTable(df):
    # Clear existing rows in the treeview
    for row in tree.get_children():
        tree.delete(row)

    # Insert new rows from the dataframe
    for index, row in df.iterrows():
        tree.insert("", "end", values=(row['Name'], row['Phone'], row['Website'], row['Locality']))

# Initialize the main window
ctk.set_appearance_mode("System")  # Modes: "System" (default), "Dark", "Light"
ctk.set_default_color_theme("blue")  # Change theme colors here

root = ctk.CTk()
root.title("YellowPages Scraper")
root.geometry("600x400")  # Set the size of the window

# Create and place the input fields
ctk.CTkLabel(root, text="Profession:").grid(row=0, column=0, padx=10, pady=5)
profession_input = ctk.CTkEntry(root)
profession_input.grid(row=0, column=1, padx=10, pady=5)

ctk.CTkLabel(root, text="Location:").grid(row=1, column=0, padx=10, pady=5)
location_input = ctk.CTkEntry(root)
location_input.grid(row=1, column=1, padx=10, pady=5)

ctk.CTkLabel(root, text="State Code:").grid(row=2, column=0, padx=10, pady=5)
state_input = ctk.CTkEntry(root)
state_input.grid(row=2, column=1, padx=10, pady=5)

# Button to run the spider
run_button = ctk.CTkButton(root, text="Run Scraper", command=runSubprocess)
run_button.grid(row=3, column=0, columnspan=2, pady=10)

# Create a table to display the CSV data
columns = ('Name', 'Phone', 'Website', 'Locality')
tree = ttk.Treeview(root, columns=columns, show='headings', height=10)
tree.heading('Name', text='Name')
tree.heading('Phone', text='Phone')
tree.heading('Website', text='Website')
tree.heading('Locality', text='Locality')

# Place the table in the window
tree.grid(row=4, column=0, columnspan=2, padx=10, pady=10)

# Start the CustomTkinter event loop
root.mainloop()
